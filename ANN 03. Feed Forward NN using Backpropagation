import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def tanh(z):
    return np.tanh(z)

def initialize_parameters(input_size, hidden_size, output_size):
    np.random.seed(42)
    W1 = np.random.randn(hidden_size, input_size)
    b1 = np.zeros((hidden_size, 1))
    W2 = np.random.randn(output_size, hidden_size)
    b2 = np.zeros((output_size, 1))
    return W1, b1, W2, b2

def forward_propagation(X, W1, b1, W2, b2, activation_func):
    Z1 = np.dot(W1, X) + b1
    A1 = tanh(Z1) if activation_func == 'tanh' else sigmoid(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)
    return Z1, A1, Z2, A2

def compute_loss(Y, A2):
    m = Y.shape[1]
    loss = -np.sum(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8)) / m
    return loss

def backward_propagation(X, Y, Z1, A1, A2, W2, activation_func):
    m = Y.shape[1]
    dZ2 = A2 - Y
    dW2 = np.dot(dZ2, A1.T) / m
    db2 = np.sum(dZ2, axis=1, keepdims=True) / m
    
    if activation_func == 'tanh':
        dA1 = 1 - A1**2
    else:
        dA1 = A1 * (1 - A1)

    dZ1 = np.dot(W2.T, dZ2) * dA1
    dW1 = np.dot(dZ1, X.T) / m
    db1 = np.sum(dZ1, axis=1, keepdims=True) / m
    return dW1, db1, dW2, db2

def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    return W1, b1, W2, b2

def train(X, Y, hidden_size, output_size, learning_rate, num_epochs, activation_func='tanh', early_stop_thresh=1e-4):
    input_size = X.shape[0]
    W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size, output_size)
    losses = []

    for epoch in range(num_epochs):
        Z1, A1, Z2, A2 = forward_propagation(X, W1, b1, W2, b2, activation_func)
        loss = compute_loss(Y, A2)
        losses.append(loss)

        if epoch > 1 and abs(losses[-2] - loss) < early_stop_thresh:
            print(f"Early stopping at epoch {epoch}, Loss stabilized at {loss:.5f}")
            break

        dW1, db1, dW2, db2 = backward_propagation(X, Y, Z1, A1, A2, W2, activation_func)
        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)

        if epoch % 1000 == 0:
            print(f"Epoch {epoch}, Loss: {loss:.5f}")

    # Plot loss
    plt.plot(losses)
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Training Loss Curve")
    plt.show()
    
    return W1, b1, W2, b2

def predict(X, W1, b1, W2, b2, activation_func='tanh'):
    _, _, _, A2 = forward_propagation(X, W1, b1, W2, b2, activation_func)
    return np.round(A2)

# XOR dataset
X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])
Y = np.array([[0, 1, 1, 0]])

# Hyperparameters
hidden_size = 4
output_size = 1
learning_rate = 1.0
num_epochs = 10000
activation_func = 'tanh'

# Train
W1, b1, W2, b2 = train(X, Y, hidden_size, output_size, learning_rate, num_epochs, activation_func)

# Predict
predictions = predict(X, W1, b1, W2, b2, activation_func)
print("Predictions:", predictions)

# Accuracy
accuracy = np.mean(predictions == Y) * 100
print(f"Accuracy: {accuracy:.2f}%")


'''
An epoch in machine learning refers to one complete pass through the entire training dataset. In the context of training a neural network, during each epoch, the following steps typically occur:

Forward Propagation: The network makes predictions (outputs) based on the current weights and biases.

Loss Calculation: The error or loss is computed by comparing the predictions to the actual labels.

Backward Propagation: The gradients of the loss with respect to the weights and biases are calculated using backpropagation.

Parameter Update: The weights and biases are adjusted using the computed gradients (usually with gradient descent or a variant).

So, an epoch is one cycle through all these steps for the entire training dataset. The more epochs you run, the better the model can learn from the data, though after a certain number of epochs, the model might start to overfit, meaning it starts memorizing the training data rather than generalizing to new data.

In your code, the model is set to train for 10,000 epochs:

python
Copy
Edit
num_epochs = 10000
This means the training process will go through the dataset 10,000 times. The model's performance (measured by loss) is typically monitored during these epochs to ensure it is improving and to decide when to stop the training process.

Why Choose Multiple Epochs?
Underfitting: If the number of epochs is too small, the model may not learn enough from the data and may underperform (underfitting).

Overfitting: If the number of epochs is too large, the model might overfit the training data, meaning it will perform well on the training set but poorly on unseen data.

You can monitor the loss over epochs to decide if the training is progressing well, and in practice, the training can be stopped early if the loss starts to stabilize or increase (i.e., when overfitting occurs).

'''
